# imports
from hashlib import new
from lib2to3.pgen2 import token
from operator import ne
import os
import torch
import transformers
from transformers import GPT2Tokenizer
from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch
from trl.ppo import PPOTrainer
from transformers import GPT2Tokenizer, AutoConfig, AutoModelForCausalLM
from transformers import AutoModel
from happytransformer import HappyGeneration
import torch.nn.functional as F
from torch import nn
from transformers import AutoModel, AutoTokenizer
import math
import torch
from transformers import PegasusForConditionalGeneration, PegasusTokenizer
from carp_model.carp_util import compute_logit
from carp_model.carp_model import ContrastiveModel, TextEncoder
from util.utils import get_model_path
from trl.gptneo import GPTNeoHeadWithValueModel
import re
from dataset.prompt_generation import generate_prompts
import matplotlib.pyplot as plt
import numpy as np

#Testing model generate
def test_1():
	#Model output
	#For some reason installing happytransformer seems to allow these imports to work
	model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")
	tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
	query_txt = "This morning I went to the "
	#return_tensors=pt means pytorch
	#encode plus returns mask for sequence classification and overflowing elements
	#in addition to encode return
	query_tensor = tokenizer.encode_plus(query_txt, return_tensors="pt")
	print(query_tensor)
	##**dict unpacks dict into keyword arguments for function call
	#Not able to call generate if loading from AutoModel.
	#AutoModel vs AutoModelForCausalLM somehow diff
	output = model.generate(
			**query_tensor,
			num_beams = 3,
			repetition_penalty = 1.2,
			no_repeat_ngram_size = 4,
			early_stopping = True,
			num_return_sequences = 3
		)
	print(output)
	#output[0] for batch
	decoded_output = tokenizer.decode(output[0])
	print(decoded_output)

#Testing model output
def test_2():
	model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")
	tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
	query_txt = "This morning I went to the "
	query_tensor = tokenizer.encode_plus(query_txt, return_tensors="pt")
	print(query_tensor)
	#output: CausalLMOutputWithPast is subclass of HF ModelOutput
	output = model(query_tensor['input_ids'])
	logits = output.logits #(batch_size, seq length, vocab_size)
	probs = F.softmax(logits[:, -1, :])
	print(probs.shape)
	#Multinomial sampling
	next_token = torch.multinomial(probs, num_samples=1).squeeze(1)
	input_ids = torch.cat([query_tensor['input_ids'], next_token.unsqueeze(-1)], dim=-1)
	decoded_output = tokenizer.decode(input_ids[0])
	decoded_next_token = tokenizer.decode(next_token)
	print(decoded_output)
	print(decoded_next_token)

def test_get_model_path():
	model_path = get_model_path("CARP_L.pt")
	print(model_path)

#carp test
def carp_test():
	model = ContrastiveModel(TextEncoder(), TextEncoder())
	ckpt_path = get_model_path("CARP_L.pt")
	model.load_state_dict(torch.load(ckpt_path))
	device = 'cuda'
	model.to(device)

	stories = [
           ["William really wanted a kilogram of spaghetti, so he went to the store. At the store he saw his best friend Jessica. Jessica and William both bought chocolate chip cookies."],
           ["According to all known laws of aviation, there is no way a bee should be able to fly. Its wings are too small to get its fat little body off the ground. The bee, of course, flies anyway because bees don't care what humans think is impossible."],
           ["Once upon a time, there lived a monster named EleutherAI, and he was really cute (in all the ways a monster of his power should be). He was quite a nice little boy and everyone loved and cared for him, even the scary."]
	]

	print(model.logit_scale)

	reviews = [
	"This kind of drags on.",
	"This is a bit too short.",
	"This is too cheery.",
	"This is really depressing.",
	"This is really exciting.",
	"This is boring.",
	"This ending leaves things too open.",
	"This ending feels abrupt.",
	"Could use more visual imagery.",
	"This is about bees.",
	"This story is generated by an AI.",
	"This story is weird.",
	"This would be good at show and tell."
	]


	#For every story, embed it and compute the cosine simarity of it against the embedded critique
	for story in stories:
		print(story)
		#Iterate over all tuples and determine which apply to this case
		for pair in reviews:
			print(pair)
			compute_logit(story, pair, model, pairs=False)
			return

def pretrained_gpt_neo_test():
	#model = GPTNeoHeadWithValueModel.from_pretrained("EleutherAI/gpt-neo-125M")
	gpt2_model = GPT2HeadWithValueModel.from_pretrained('gpt2')

def regex_test():
	sample_text = 'Once upon a time   \nwhen the stars were still in the sky.\n\nA'
	new_sample_text = re.sub('\s\s+', " ", sample_text)
	new_sample_text = re.sub('\n', '', new_sample_text)
	print(sample_text)
	print(new_sample_text)

def generation_test():
	generate_prompts()

def test_fine_tuned_lm():
	model_path = get_model_path('model.pt')
	model = GPT2HeadWithValueModel.from_pretrained("lvwerra/gpt2-imdb")
	model.to('cuda')
	model.load_state_dict(torch.load(model_path))
	tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
	query_txt = ["This morning I went to the "]
	batch = query_txt
	batch_token =[tokenizer.encode(x, return_tensors="pt").to('cuda')[0, :5] for x in batch]
	query_tensors = torch.stack(batch_token)

	response_tensors = respond_to_batch(model, query_tensors, txt_len=15)
	stories = [tokenizer.decode(response_tensors[i, :]) for i in range(len(response_tensors))]
	print(stories)

def test_plotting():
	y = [torch.tensor(1), torch.tensor(2), torch.tensor(3)]
	x = np.arange(len(y))
	plt.clf()
	plt.plot(x,y)
	plt.savefig('scores.png')


if __name__=='__main__':
	test_fine_tuned_lm()