{
  #LM Model args
  "lm_name": "gpt2-large",
  "ref_lm_name": "gpt2-large",
  "tk_name": "gpt2-large",
  "num_layers_unfrozen": 2,
  "save_model": True,
  "save_folder": 'ckpts/aligned_default_model/',

  #Carp model args
  "carp_version": "default",
  "carp_config_path": "/home/ubuntu/alex/pref_learning/magiCARP/configs/carp_l.yml",
  "carp_ckpt_path": "/home/ubuntu/alex/pref_learning/magiCARP/ckpts/CARP Roberta L/",

  #Training args
  "steps": 20000,
  "batch_size": 64,
  "forward_batch_size": 16,

  #PPO Args
  "ppo_epochs": 4,
  "txt_in_len": 14,
  "txt_out_len": 60,
  "lr": 1.41e-5,
  "init_kl_coef":0.2,
  "target": 50,   #KL Divergence target
  "horizon":10000,
  "gamma":1,   #Discount factor
  "lam":0.95,
  "cliprange": .2,
  "cliprange_value":.2,
  "vf_coef":.2,

  #Review
  "review": "Make this a story about a good person",

  #Dataset
  "data_path": "dataset/alt_prompts.txt",

  #Logging
  'LOG': True,

  #Evaluation
  'EVAL': True,
  "num_eval_examples": 10,

  #Minimize or maximize
  'minimize': False,
}